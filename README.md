# STIMA Machine Learning Reading Group
The reading group is open to anyone who is interested in machine learning and who wants to meet up regularly to discuss ML research papers.
If you want to get added to the mailing list or have any other questions feel free to contact [Joel Oskarsson](https://liu.se/en/employee/joeos82).

For VT 2024 we will continue with the format where we stick to papers on the same topic for two subsequent sessions. The first session (January 17) will be devoted to deciding all the topics for the fall.

## Format
* One or two people are designated the host for each topic (two sessions). They are responsible for choosing the papers to be discussed and for leading the discussion during the session.
* We meet Wednesdays 10:15-12:00 on even weeks.
* We will meet in Thomas Bayes, B-building ([map](https://www.ida.liu.se/department/location/search.en.shtml?keyword=thomas+bayes)).

## Host intstructions
* Choose one paper for each session related to your topic that you think would be interesting to discuss in the reading group. In order to set a focus for the reading group we came up with the following short guidelines for how to choose papers:
  * The main topic of the paper should be core machine learning research. Try to avoid papers that just apply well known machine learning methods to specific application areas.
  * Make sure the paper is of high quality. Read through it yourself and try to gauge its quality. As a guideline, think that it should be publishable at a top machine learning conference (i.e. the paper should be of such quality, it does not have to actually be a short conference paper).
  * If you want a second opinion on whether a paper is suitable feel free to ask anyone who has been in the reading group previous years.
* Think about how the two papers in your topic relate to each other. For example, it can be nice to discuss first an introductory paper and then the state-of-the-art, or two different approaches to/perspectives on the same underlying problem.
* Send out a link to the paper on the mailing list at least one week in advance.
* As the host it is also good to somewhat lead the discussion during the session. If you want you can give a short description of why you chose this paper, but there is no need for any proper presentation. It might be a good idea to come to the session prepared with a few discussion points, just to keep the conversation going.

## Schedule VT 2024

__Week 3 (Jan 17)__ (*NOTE:* Odd week)

Decide on topics for the spring.

__Week 4 (Jan 24)__
<br>
__Topic: Diffusion Models__
_- Host:_ Theodor, Filip, Dong

*Elucidating the Design Space of Diffusion-Based Generative Models*
<br>
Tero Karras, Miika Aittala, Timo Aila, Samuli Laine
<br>
https://arxiv.org/abs/2206.00364

Our rating: 3.67 ± 0.47

__Week 6 (Feb 7)__ (*NOTE:* Change of location to John von Neumann ([map](https://www.ida.liu.se/department/location/search.en.shtml?keyword=von+neumann)))
<br>
__Topic: Diffusion Models__
_- Host:_ Theodor, Filip, Dong

*Consistency Models*
<br>
Yang Song, Prafulla Dhariwal, Mark Chen, Ilya Sutskever
<br>
https://proceedings.mlr.press/v202/song23a

*Related:* [Presentation by first author](https://neurips.cc/virtual/2023/workshop/66539) (starts at around 5:32:30).

Our rating: 3.2 ± 0.75

__Week 8 (Feb 21)__
<br>
__Topic: Diffusion Models__
_- Host:_ Theodor, Filip, Dong

*Practical and Asymptotically Exact Conditional Sampling in Diffusion Models*
<br>
Luhuan Wu, Brian L. Trippe, Christian A Naesseth, David Blei, John Patrick Cunningham 
<br>
https://openreview.net/forum?id=eWKqr1zcRv

Our rating: 2.67 ± 1.25

__Week 10 (Mar 6)__
<br>
__Topic: Neural Operators__
_- Host:_ Joel, Samuel

*Fourier Neural Operator for Parametric Partial Differential Equations*
<br>
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar
<br>
https://arxiv.org/abs/2010.08895

Our rating: 3.33 ± 0.47

__Week 12 (Mar 20)__
<br>
*Learning Space-Time Continuous Latent Neural PDEs from Partially Observed States*
<br>
Valerii Iakovlev, Markus Heinonen, Harri Lähdesmäki
<br>
https://proceedings.neurips.cc/paper_files/paper/2023/hash/53e9b4152ca09d5f1228157e752651dd-Abstract-Conference.html

__Week 14 (Apr 3)__
<br>
__Topic: Neural Operators__
_- Host:_ Joel, Samuel

*Spherical Fourier Neural Operators: Learning Stable Dynamics on the Sphere*
<br>
Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, Anima Anandkumar
<br>
https://arxiv.org/abs/2306.03838

Our rating: 2.50 ± 0.50

__Week 16 (Apr 17)__
<br>
__Topic: Transformers, beyond AIAYN__
_- Host:_ Amanda, Hari

*Mamba: Linear-Time Sequence Modeling with Selective State Spaces*
<br>
Albert Gu, Tri Dao
<br>
https://arxiv.org/abs/2312.00752

Our rating: 2.20 ± 0.97

__Week 20 (May 15)__
<br>
_guest:_ Yingzhen Li
<br>
*Sparse Uncertainty Representation in Deep Learning with Inducing Weights*
<br>
Hippolyt Ritter, Martin Kukla, Cheng Zhang, Yingzhen Li
<br>
https://arxiv.org/abs/2105.14594

__Week 22 (May 29)__
<br>
__Topic: Transformers, beyond AIAYN__
_- Host:_ Amanda, Hari

*xLSTM: Extended Long Short-Term Memory*
<br>
Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, Sepp Hochreiter
<br>
https://arxiv.org/abs/2405.04517

Our rating: 1.29 ± 0.45

__Week 24 (Jun 12)__
<br>
__Topic: Transformers, beyond AIAYN__
_- Host:_ Amanda, Hari

## Earlier sessions

* [HT 2023](archive/2023ht.md)
* [VT 2023](archive/2023vt.md)
* [HT 2022](archive/2022ht.md)
* [VT 2022](archive/2022vt.md)
* [VT 2021](archive/2021vt.md)
* [VT 2020](archive/2020vt.md)

## Paper scoring scale

5: Very Strong Accept:

* Technically flawless paper
* with groundbreaking impact on at least one area of ML and excellent impact on multiple areas of ML,
* with flawless evaluation, resources, and reproducibility,
* and no unaddressed ethical considerations.

4: Strong Accept:

* Technically strong paper, with novel ideas,
* excellent impact on at least one area of ML or high-to-excellent impact on multiple areas of ML,
* with excellent evaluation, resources, and reproducibility,
* and no unaddressed ethical considerations.

3: Accept:

* Technically solid paper,
* with high impact on at least one sub-area of ML or moderate-to-high impact on more than one area of ML,
* with good-to-excellent evaluation, resources, reproducibility,
* and no unaddressed ethical considerations.

2: Weak Accept:

* Technically solid,
* moderate-to-high impact paper,
* with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.

1: Borderline accept:

* Technically solid paper
* where reasons to accept outweigh reasons to reject, e.g., limited evaluation.
