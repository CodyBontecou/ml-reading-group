# STIMA Machine Learning Reading Group
The reading group is open to anyone who is interested in machine leraning and who wants to meet up regularly to discuss ML research papers. Students who actively participate in the reading group can also take it as a PhD course worth 3 credits for one semester (details below).

## Format
- We meet (roughly) bi-weekly on Tuesdays 13:15-15:00 throughout the semester to discuss papers in machine learning.
- The paper selection is based on certain topics that we decide on together during the first session (a brainstorming session), so the topics covered during each semester (i.e. course instance) will be based on the participants' interests. We will stick with one topic for two consecutive sessions.
- Two people will share the responsibility for selecting good and representative papers for each topic. You can either collaborate on the two sessions, or take one session each.
- Each session we will have one main paper that everyone should read, and possibly a few more related papers.
- Each session will start with a presentation by the person(s) who is responsible for that session, focusing on putting the main paper in a context. For instance, you can present the key idea of the main paper and how this differs from the related papers that you have picked. 
- The presentation is followed by a discussion.
- If you present on week X, inform Fredrik about your selected paper(s) no later than Friday noon on week X-2. Everyone else will be notified on Tuesday on week X-1 (at the latest), giving you one week to read the paper(s) before the discssion.

PhD students who want to earn three course credits (3hp) need to be responsible for one session/topic, and actively participate in at least 70 % of the sessions.

## Dates VT 2021
- First meeting (brainstorming session) on Jan 19.
- First regular session on Feb 2.
- In total 10 regular sessions (odd weeks throughout the semester). You need to attend at least 7 of these to earn the course credits.

## Selected topics VT 2021

__Week 5 (Feb 2): Meta-learning / few-shot learning__
_- Responsible:_ FE, AmA

Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
<br>
Chelsea Finn, Pieter Abbeel, Sergey Levine
<br>
Proceedings of the 34th International Conference on Machine Learning, PMLR 70:1126-1135, 2017.
<br>
http://proceedings.mlr.press/v70/finn17a.html

__Week 7 (Feb 16): Meta-learning / few-shot learning__
_- Responsible:_ FE, AmA


__Week 9 (Mar 2): Explainability/interpretability__
_- Responsible:_ JF, HM


__Week 12 (Mar 23): Explainability/interpretability__
_- Responsible:_ JF, HM


__Week 14 (Apr 6): Self-supervised learning / contrastive learning__
_- Responsible:_ AzA, JO


__Week 16 (Apr 20): Self-supervised learning / contrastive learning__
_- Responsible:_ AzA, JO


__Week 18 (May 4): Normalizing flows (for variational inference)__
_- Responsible:_ SB, HG


__Week 20 (May 18): Normalizing flows (for variational inference)__
_- Responsible:_ SB, HG


__Week 22 (Jun 1): Time series forecasting with ML__
_- Responsible:_ JW, MFS


__Week 24 (Jun 15): Time series forecasting with ML__
_- Responsible:_ JW, MFS



## Topics discussed VT 2020

__Feb 4: Attention 1 (FL)__ 

_Main discussion paper:_ Xu et al. (2015) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, https://arxiv.org/abs/1502.03044

_Additional papers:_
* Bahdanau et al. (2014), Neural Machine Translation by Jointly Learning to Align and Translate, https://arxiv.org/abs/1409.0473
* Vaswani et al. (2017) Attention Is All You Need, https://arxiv.org/abs/1706.03762
* Chaudhari et al. (2019) An Attentive Survey of Attention Models, https://arxiv.org/abs/1904.02874

__Feb 18: Attention 2 (HG)__

_Main discussion paper:_ Vaswani et al. (2017) Attention is All You Need, https://arxiv.org/abs/1706.03762

_Follow-up paper:_ Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805

__Mar 3: Out-of-distribution 1 (AA,JO)__

_Main discussion paper:_ https://arxiv.org/abs/1807.02588

_Additional papers:_ 
* https://arxiv.org/abs/1910.04241
* https://arxiv.org/abs/1511.05644

__Mar 17:__  _Canceled_

__Mar 31: Out-of-distribution 2 (AE)__

_Main discussion paper:_
* What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?
https://arxiv.org/abs/1703.04977

_Related papers:_
* Sampling-free Epistemic Uncertainty Estimation Using Approximated Variance Propagation
https://arxiv.org/abs/1908.00598
* Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow
https://arxiv.org/abs/1802.07095

__Apr 14: Gaussian processes + deep learning 1 (AO,AK,JW)__
_E.g. Deep GP, Deep Kernel Learning_

_Main discussion paper:_
* Deep Kernel Learning http://proceedings.mlr.press/v51/wilson16.pdf.

_Related papers:_
* Gaussian Process Kernels for Pattern Discovery and Extrapolation http://proceedings.mlr.press/v28/wilson13.pdf (explaining the spectral mixture base kernel)
* Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)  http://proceedings.mlr.press/v37/wilson15.pdf (explaining the KISS-GP approach used for covariance approximation)

__Apr 28: Gaussian processes + deep learning 2 (AO,AK,JW)__
_E.g. Deep GP, Deep Kernel Learning_

* Stochastic Variational Deep Kernel Learning https://papers.nips.cc/paper/6426-stochastic-variational-deep-kernel-learning.pdf
* Learning Scalable Deep Kernels with Recurrent Structure http://jmlr.org/papers/volume18/16-498/16-498.pdf

If you are interested in learning more about LSTMs (for the second paper), you could also have a look at:
https://www.researchgate.net/publication/13853244_Long_Short-term_Memory


__May 12: Graph Neural Networks 1 (MM, FE, JK)__

* Main paper: Semi-Supervised Classification with Graph Convolutional Networks https://arxiv.org/abs/1609.02907

* Additional reading/background paper: Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering https://papers.nips.cc/paper/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.pdf

 
__May 26: Graph Neural Networks 2 (MM, FE, JK)__


* Main paper: How Powerful are Graph Neural Networks? https://cs.stanford.edu/people/jure/pubs/gin-iclr19.pdf

* Additional reading/background paper: Inductive Representation Learning on Large Graphs https://cs.stanford.edu/people/jure/pubs/graphsage-nips17.pdf



__Jun 9: Normalizing flows and GANs 1 (FJL, AR, GH)__

* Normalizing Flows: An Introduction and Review of Current Methods
https://arxiv.org/abs/1908.09257

* Glow: Generative Flow with Invertible 1x1 Convolutions
http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-con


__Jun 23: Normalizing flows and GANs 2 (FJL, AR, GH)__

* Improved Training of Wasserstein GANs
https://papers.nips.cc/paper/7159-improved-training-of-wasserstein-gans

GAN background papers:

* Generative adversarial nets
http://papers.nips.cc/paper/5423-generative-adversarial-nets

* Wasserstein Generative Adversarial Networks
http://proceedings.mlr.press/v70/arjovsky17a.html

