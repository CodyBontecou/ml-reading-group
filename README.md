# STIMA Machine Learning Reading Group
The reading group is open to anyone who is interested in machine leraning and who wants to meet up regularly to discuss ML research papers. Students who actively participate in the reading group can also take it as a PhD course worth 3 credits for one semester (details below).

## Format
- We meet bi-weekly on Tuesdays 13:15-15:00 throughout the semester to discuss papers in machine learning.
- The paper selection is based on certain topics that we decide on together during the first session (a brainstorming session), so the topics covered during each semester (i.e. course instance) will be based on the participants' interests. We will stick with one topic for two consecutive sessions.
- Two people will share the responsibility for selecting good and representative papers for each topic. You can either collaborate on the two sessions, or take one session each.
- Each session we will have one main paper that everyone should read, and possibly a few more related papers.
- Each session will start with a presentation by the person(s) who is responsible for that session, focusing on putting the main paper in a context. For instance, you can present the key idea of the main paper and how this differs from the related papers that you have picked. 
- The presentation is followed by a discussion. 

PhD students who want to earn three course credits (3hp) need to be responsible for one session/topic, and actively participate in at least 70 % of the sessions.

## Dates VT 2020
- First meeting (brainstorming session) on Jan 21.
- First regular session on Feb 4.
- In total 10 regular sessions (even weeks throughout the semester). You need to attend at least 7 of these to earn the course credits.

## Selected topics VT 2020

__Feb 4: Attention 1 (FL)__ 

_Main discussion paper:_ Xu et al. (2015) Show, Attend and Tell: Neural Image Caption Generation with Visual Attention, https://arxiv.org/abs/1502.03044

_Additional papers:_
* Bahdanau et al. (2014), Neural Machine Translation by Jointly Learning to Align and Translate, https://arxiv.org/abs/1409.0473
* Vaswani et al. (2017) Attention Is All You Need, https://arxiv.org/abs/1706.03762
* Chaudhari et al. (2019) An Attentive Survey of Attention Models, https://arxiv.org/abs/1904.02874

__Feb 18: Attention 2 (HG)__

_Main discussion paper:_ Vaswani et al. (2017) Attention is All You Need, https://arxiv.org/abs/1706.03762

_Follow-up paper:_ Devlin et al. (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, https://arxiv.org/abs/1810.04805

__Mar 3: Out-of-distribution 1 (AA,JO)__

_Main discussion paper:_ https://arxiv.org/abs/1807.02588

_Additional papers:_ 
* https://arxiv.org/abs/1910.04241
* https://arxiv.org/abs/1511.05644

__Mar 17:__  _Canceled_

__Mar 31: Out-of-distribution 2 (AE)__

__Apr 14: Gaussian processes + deep learning 1 (AO,AK,JW)__
_E.g. Deep GP, Deep Kernel Learning_

__Apr 28: Gaussian processes + deep learning 2 (AO,AK,JW)__
_E.g. Deep GP, Deep Kernel Learning_

__May 12: Graph Neural Networks 1 (MM, FE, JK)__

__May 26: Graph Neural Networks 2 (MM, FE, JK)__

__Jun 9: ????? 1 (FJL, AR, GH)__

__Jun 23: ????? 2 (FJL, AR, GH)__


